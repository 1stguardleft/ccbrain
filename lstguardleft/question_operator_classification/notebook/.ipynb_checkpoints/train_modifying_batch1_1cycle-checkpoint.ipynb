{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import preprocessor as pr\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "from konlpy.tag import Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO 6.1 : TODO 1 ~ 5 까지 데이터 Load (using by Pickle)\n",
    "vocab_dict = pr.load_pickle('./data/question_dict.pickle')\n",
    "operator_dict = pr.load_pickle('./data/operator_dict.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim  = x_num_classes  = n_words = len(vocab_dict)               # 모두 2048 \n",
    "output_dim = y_num_classes = hidden_size = len(operator_dict)         # 모두   99\n",
    "# lstm_size: Number of units in the hidden layers in the LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512. etc\n",
    "lstm_size = hidden_dim = 512\n",
    "# lstm_layers: Number of LSTM layers in the network. I'd start with 1, then add more if I'm underfitting.\n",
    "lstm_layers = 1\n",
    "# batch_size: The number of reviews to feed the network in one training pass. Typically this should be set as high as you can go without running out of memory\n",
    "\n",
    "epochs        = 10  # 1 epoch = one forward pass and one backward pass of all the  training examples.\n",
    "learning_rate = 0.02\n",
    "batch_size    = 100  # batch size = the number of training examples in one forward/backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataX = []\n",
    "dataY = []\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sop = pd.read_csv('./data/question_operator_set_1500.csv')\n",
    "\n",
    "for idx, value in enumerate(df_sop['question']):\n",
    "    dataX.append([vocab_dict[question] for question in twitter.morphs(value)]) \n",
    "    \n",
    "for idx, value in enumerate(df_sop['operator']):\n",
    "    dataY.append(operator_dict[value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dataX : (1500,)\n",
      "shape of dataY : (1500,)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of dataX : {}\".format(np.array(dataX).shape)) # (1500, ) : 가변길이이기 때문에 정확하게 표현되지 않음\n",
    "print(\"shape of dataY : {}\".format(np.array(dataY).shape)) # (1500, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length_list = [len(val) for idx, val in enumerate(dataX)]\n",
    "sequence_length = max_len = max(sequence_length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# left Zero-padding, 단 Zero-Padding시에도 dynamic_rnn 사용함.\n",
    "# Many to One 모델 적용으로 left Zero-padding\n",
    "for idx, val in enumerate(dataX):\n",
    "    if len(dataX[idx]) < max_len:\n",
    "        for ind in range(0, max_len - len(dataX[idx])):\n",
    "            # dataX[idx].append(0)  right zero-padding\n",
    "            dataX[idx].insert(0, 0) # left zero-padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dataX : (1500, 50)\n",
      "shape of dataY : (1500,)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of dataX : {}\".format(np.array(dataX).shape)) # (1500, 50) : left Zero-padding으로 (1500, 50)\n",
    "print(\"shape of dataY : {}\".format(np.array(dataY).shape)) # (1500, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(1200, 50) \n",
      "Validation set: \t(150, 50) \n",
      "Test set: \t\t(150, 50)\n",
      "label set: \t\t(1200,) \n",
      "Validation label set: \t(150,) \n",
      "Test label set: \t(150,)\n"
     ]
    }
   ],
   "source": [
    "split_train_frac = 0.8\n",
    "\n",
    "split_index = int(split_train_frac * len(dataX))\n",
    "\n",
    "train_x, val_x = np.array(dataX[:split_index]), np.array(dataX[split_index:]) \n",
    "train_y, val_y = np.array(dataY[:split_index]), np.array(dataY[split_index:])\n",
    "\n",
    "split_test_frac = 0.5\n",
    "split_index = int(split_test_frac * len(val_x))\n",
    "\n",
    "val_x, test_x = np.array(val_x[:split_index]), np.array(val_x[split_index:])\n",
    "val_y, test_y = np.array(val_y[:split_index]), np.array(val_y[split_index:])\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
    "print(\"label set: \\t\\t{}\".format(train_y.shape), \n",
    "      \"\\nValidation label set: \\t{}\".format(val_y.shape),\n",
    "      \"\\nTest label set: \\t{}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch-size = 1을 위한 훈련 데이터 변경 (iterations : 10)\n",
    "#train_x, train_y = train_x[: 10], train_y[: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntmp_x, tmp_y = [], []\\nfor idx in range(10):\\n    tmp_x, tmp_y = np.append(tmp_x, train_x), np.append(tmp_y, train_y)\\ntrain_x, train_y = np.reshape(tmp_x, [-1, 50]), np.reshape(tmp_y, [-1])\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 동일 데이터를 학습시켜보기\n",
    "'''\n",
    "tmp_x, tmp_y = [], []\n",
    "for idx in range(10):\n",
    "    tmp_x, tmp_y = np.append(tmp_x, train_x), np.append(tmp_y, train_y)\n",
    "train_x, train_y = np.reshape(tmp_x, [-1, 50]), np.reshape(tmp_y, [-1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 동일 데이터를 통해 validation\n",
    "val_x = train_x[1050 :, :]\n",
    "val_y = train_y[1050 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output 정규화 >> tf.conrib.layer.bat_norm 사용 방법 확인해볼 것\n",
    "# train_y = train_y / max(train_y) - min(train_y)\n",
    "# val_y = val_y / max(val_y) - min(val_y)\n",
    "# test_y = test_y / max(test_y) - min(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the graph object\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Logging 설정 - \n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "with tf.name_scope('Input'):\n",
    "    X = tf.placeholder(tf.int32, [None, None], name=\"X\")\n",
    "    Y = tf.placeholder(tf.int32, [None], name=\"Y\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Input/X:0\", shape=(?, ?), dtype=int32)\n",
      "Tensor(\"Input/Y:0\", shape=(?,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "embed_size = 300 \n",
    "\n",
    "with tf.name_scope(\"Embeddings\"):\n",
    "    embedding_x = tf.Variable(tf.random_uniform((x_num_classes, embed_size), -1, 1))\n",
    "    embed_x = tf.nn.embedding_lookup(embedding_x, X)\n",
    "\n",
    "#X_one_hot = tf.one_hot(X, x_num_classes)\n",
    "Y_one_hot = tf.one_hot(Y, y_num_classes, 1, 0)\n",
    "#Y_one_hot_rs = tf.reshape(Y_one_hot, [-1, 99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_x : Tensor(\"Embeddings/Variable/read:0\", shape=(2047, 300), dtype=float32)\n",
      "embed_x : Tensor(\"Embeddings/embedding_lookup:0\", shape=(?, ?, 300), dtype=float32)\n",
      "Y_one_hot : Tensor(\"one_hot:0\", shape=(?, 99), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(\"embedding_x : \" + str(embedding_x)) \n",
    "print(\"embed_x : \" + str(embed_x)) # (?, ?, 300)\n",
    "print(\"Y_one_hot : \" + str(Y_one_hot)) # (?, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell():\n",
    "    # Your basic LSTM cell\n",
    "    #lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size, reuse=tf.get_variable_scope().reuse)\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    # Add dropout to the cell\n",
    "    return tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"RNN_layers\"):\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(lstm_layers)])\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LSTMStateTuple(c=<tf.Tensor 'RNN_layers/zeros:0' shape=(100, 512) dtype=float32>, h=<tf.Tensor 'RNN_layers/zeros_1:0' shape=(100, 512) dtype=float32>),)\n"
     ]
    }
   ],
   "source": [
    "print(initial_state) # shape=(100, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"RNN_forward\"):\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, embed_x, initial_state=initial_state)\n",
    "#cell = rnn.BasicLSTMCell(hidden_dim, state_is_tuple = True)\n",
    "#initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "#outputs, _states = tf.nn.dynamic_rnn(cell, X_one_hot, sequence_length=sequence_length_list, dtype=tf.float32)\n",
    "#outputs, _states = tf.nn.dynamic_rnn(cell, X_one_hot, initial_state=initial_state, dtype=tf.float32)\n",
    "#rnn_outputs, _states = tf.nn.dynamic_rnn(cell, X_one_hot, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((LSTMStateTuple(c=<tf.Tensor 'RNN_forward/rnn/while/Exit_2:0' shape=(100, 512) dtype=float32>, h=<tf.Tensor 'RNN_forward/rnn/while/Exit_3:0' shape=(100, 512) dtype=float32>),),\n",
       " <tf.Tensor 'one_hot:0' shape=(?, 99) dtype=int32>,\n",
       " <tf.Tensor 'RNN_forward/rnn/transpose:0' shape=(100, ?, 512) dtype=float32>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state, Y_one_hot, rnn_outputs  # shape=(1, 512) shape=(1, 512) shape=(1, ?, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#funciton to transform gradients\n",
    "def transform_grad(grad, decay=1.0):\n",
    "    #return decayed gradient\n",
    "    return decay*grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('predictions'):\n",
    "    fc_output = tf.contrib.layers.fully_connected(rnn_outputs[:, -1], y_num_classes, activation_fn=None)\n",
    "    tf.summary.histogram('logits', fc_output)\n",
    "    \n",
    "with tf.name_scope('cost'):\n",
    "    #cost = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y_one_hot)\n",
    "    cost = tf.nn.softmax_cross_entropy_with_logits(logits=fc_output, labels=tf.to_float(Y_one_hot))\n",
    "    cost = tf.reduce_sum(cost)\n",
    "    tf.summary.scalar('cost', cost)\n",
    "    \n",
    "# Defintion of cross entropy loss function\n",
    "# D(y, y^hat) = - Sum(y element-wise multiplcation log (y^hat))\n",
    "    \n",
    "    calculated_loss = -tf.reduce_sum(tf.to_float(Y_one_hot) * tf.log(tf.nn.softmax(fc_output)))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    \n",
    "    #opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    \n",
    "    # gradient variable list = [ (gradient,variable) ]\n",
    "    #gv = opt.compute_gradients(cost)\n",
    "    \n",
    "    # transformed gradient variable list = [ (T(gradient),variable) ]\n",
    "    #decay = 0.9 # decay the gradient for the sake of the example\n",
    "    #tgv = [ (g, v) for (g,v) in gv] #list [(grad,var)]\n",
    "    #tgv = [ (T(g,decay=decay), v) for (g,v) in gv] #list [(grad,var)]\n",
    "    \n",
    "    # apply transformed gradients (this case no transform)\n",
    "    #apply_transform_op = opt.apply_gradients(tgv)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "#opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "# gradient variable list = [ (gradient,variable) ]\n",
    "#gv = opt.compute_gradients(y,[x])\n",
    "\n",
    "# transformed gradient variable list = [ (T(gradient),variable) ]\n",
    "#decay = 0.9 # decay the gradient for the sake of the example\n",
    "#tgv = [ (T(g,decay=decay), v) for (g,v) in gv] #list [(grad,var)]\n",
    "\n",
    "# apply transformed gradients (this case no transform)\n",
    "#apply_transform_op = opt.apply_gradients(tgv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('validation'):\n",
    "    correct = tf.equal(tf.argmax(fc_output,1), tf.argmax(Y_one_hot,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Feed Data의 차원 ====\n",
      "dataX : (1500, 50)\n",
      "dataY : (1500,)\n",
      "\n",
      "==== 입력 - 출력 텐서의 차원 변화 ====\n",
      "placeholder X : (?, ?)\n",
      "placeholder Y : (?,)\n",
      "embedding_x : Tensor(\"Embeddings/Variable/read:0\", shape=(2047, 300), dtype=float32)\n",
      "embed_x : Tensor(\"Embeddings/embedding_lookup:0\", shape=(?, ?, 300), dtype=float32)\n",
      "rnn_outputs : (100, ?, 512)\n"
     ]
    }
   ],
   "source": [
    "print(\"==== Feed Data의 차원 ====\")\n",
    "print(\"dataX : \" + str(np.array(dataX).shape))\n",
    "print(\"dataY : \" + str(np.array(dataY).shape))\n",
    "\n",
    "print(\"\\n==== 입력 - 출력 텐서의 차원 변화 ====\")\n",
    "print(\"placeholder X : \" + str(X.get_shape()))\n",
    "print(\"placeholder Y : \" + str(Y.get_shape()))\n",
    "print(\"embedding_x : \" + str(embedding_x))\n",
    "print(\"embed_x : \" + str(embed_x))\n",
    "print(\"rnn_outputs : \" + str(rnn_outputs.get_shape()))\n",
    "# print(\"predictions : \" + str(predictions.get_shape()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 1 Train loss: 459.777\n",
      "Epoch: 0/10 Iteration: 2 Train loss: 438.459\n",
      "Epoch: 0/10 Iteration: 3 Train loss: 436.621\n",
      "Epoch: 0/10 Iteration: 4 Train loss: 439.046\n",
      "Epoch: 0/10 Iteration: 5 Train loss: 441.913\n",
      "Val acc: 0.060\n",
      "Epoch: 0/10 Iteration: 6 Train loss: 442.190\n",
      "Epoch: 0/10 Iteration: 7 Train loss: 432.781\n",
      "Epoch: 0/10 Iteration: 8 Train loss: 441.055\n",
      "Epoch: 0/10 Iteration: 9 Train loss: 435.522\n",
      "Epoch: 0/10 Iteration: 10 Train loss: 438.550\n",
      "Val acc: 0.070\n",
      "Epoch: 0/10 Iteration: 11 Train loss: 431.004\n",
      "Epoch: 0/10 Iteration: 12 Train loss: 434.464\n",
      "Epoch: 1/10 Iteration: 13 Train loss: 425.869\n",
      "Epoch: 1/10 Iteration: 14 Train loss: 423.040\n",
      "Epoch: 1/10 Iteration: 15 Train loss: 423.504\n",
      "Val acc: 0.080\n",
      "Epoch: 1/10 Iteration: 16 Train loss: 431.143\n",
      "Epoch: 1/10 Iteration: 17 Train loss: 430.595\n",
      "Epoch: 1/10 Iteration: 18 Train loss: 433.237\n",
      "Epoch: 1/10 Iteration: 19 Train loss: 432.088\n",
      "Epoch: 1/10 Iteration: 20 Train loss: 437.334\n",
      "Val acc: 0.070\n",
      "Epoch: 1/10 Iteration: 21 Train loss: 434.176\n",
      "Epoch: 1/10 Iteration: 22 Train loss: 430.993\n",
      "Epoch: 1/10 Iteration: 23 Train loss: 429.634\n",
      "Epoch: 1/10 Iteration: 24 Train loss: 432.959\n",
      "Epoch: 2/10 Iteration: 25 Train loss: 424.183\n",
      "Val acc: 0.050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-1cdac9cc3259>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;31m#embed_xx, yone, rnn_out, logit, loss, _ = sess.run([embed_x, Y_one_hot, rnn_outputs, logits, cost, optimizer], feed_dict=feed)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmerged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[1;31m#summary, losses, state, _, grad = sess.run([merged, cost, final_state, apply_transform_op, gv], feed_dict=feed)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with graph.as_default():\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('./logs/tb/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logs/tb/test', sess.graph)\n",
    "    \n",
    "    statelist = []\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for idx, (x, y) in enumerate(get_batches(train_x, train_y, batch_size)):\n",
    "            feed = {X: x, Y: y, keep_prob: 0.8, initial_state: state}\n",
    "            #embed_xx, yone, rnn_out, logit, loss, _ = sess.run([embed_x, Y_one_hot, rnn_outputs, logits, cost, optimizer], feed_dict=feed)\n",
    "            summary, losses, state, _ = sess.run([merged, cost, final_state, optimizer], feed_dict=feed)\n",
    "            #summary, losses, state, _, grad = sess.run([merged, cost, final_state, apply_transform_op, gv], feed_dict=feed)\n",
    "            \n",
    "            yone, logit, c_loss = sess.run([Y_one_hot, fc_output, calculated_loss], feed_dict=feed)\n",
    "            '''\n",
    "            if ii == 1:\n",
    "                print(\"shape of x : {}\".format(x.shape))\n",
    "                print(\"shape of y : {}\".format(y.shape))\n",
    "                print(\"shape of embed_xx : {}\".format(embed_xx .shape))\n",
    "                print(\"shape of yone : {}\".format(yone.shape))\n",
    "                print(\"shape of rnn_outputs : {}\".format(rnn_out.shape))\n",
    "                print(\"shape of rnn_outputs_last_step : {}\".format(rnn_out[:, -1].shape))\n",
    "                #print(\"shape of states : {}\".format(np.array(final).shape))\n",
    "            '''\n",
    "            # \n",
    "            statelist.append(state)\n",
    "            \n",
    "            train_writer.add_summary(summary, iteration)\n",
    "\n",
    "            #if iteration%1==0:\n",
    "            print(\"Epoch: {}/{}\".format(e, epochs), \"Iteration: {}\".format(iteration), \"Train loss: {:.3f}\".format(losses))\n",
    "            #print(\"dimension of grad: {}\".format(np.array(grad).shape))\n",
    "            #print(\"sum of grad: {}\".format(sum(np.array(grad)[idx])))\n",
    "            #print(\"sum of calculated loss : {}\".format(c_loss))\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                \n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {X: x, Y: y, keep_prob: 1, initial_state: val_state}\n",
    "                    summary, batch_acc, val_state = sess.run([merged, accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                    \n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "                \n",
    "            iteration +=1\n",
    "            test_writer.add_summary(summary, iteration)\n",
    "            saver.save(sess, \"checkpoints/sentiment_manish.ckpt\")\n",
    "    saver.save(sess, \"checkpoints/sentiment_manish.ckpt\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
